\chapter{实验}
\label{chap:experiment}
在实验部分，我们通过让游戏AI对战固定场次来判断其强度。也就是说，若某个玩家的胜率越高，则其强度越强。由于缺乏职业玩家与普通玩家的大量数据，我们无法使用Elo rating对AI玩家的强度进行打分\cite{glickman1999rating}。
在目前的硬件条件下，我们支持图\ref{fig:sstate}中所示4种开局，白色方总是先攻，对战双方每50局互换先手，每100局更换开局棋盘，一共对战400局。
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{sstate.PNG}
    \caption[sstate]{%
        超级皇后对战的4种开局%
      }
    \label{fig:sstate}
\end{figure}
对于随机玩家，贪婪玩家与Alpha-beta剪枝玩家这三个基准玩家而言，在任意规则下，其强度排序为Alpha-beta剪枝玩家$>$贪婪玩家$>$随机玩家。具体对局数据如图\ref{fig:baseresult}所示，先手与后手的对战结果合并计算，包含骑士、皇后与超级皇后三种规则状态，随机玩家简写作RP，贪婪玩家简写作GP，Alpha-beta剪枝玩家简写作ABP。
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\textwidth]{baseplayer.PNG}
    \caption[baseresult]{%
        基准玩家测试%
      }
    \label{fig:baseresult}
\end{figure}
从基准玩家测试结果中，我们发现贪婪玩家与Alpha-beta剪枝玩家不能完全碾压随机玩家，随机玩家总能赢少部分局。可能的原因是在有限时间内Alpha-beta剪枝层数不够深，陷入了局部最优解，封死了自己在后续的优势。当然也可能是因为在游戏中后期棋盘状态复杂度降低，随机玩家可以走出关键棋从而制胜。
整体上来看，Alpha-beta剪枝玩家对随机玩家胜率在$90\%$，对贪婪玩家胜率在$75\%$，贪婪玩家对随机玩家胜率在$70\%$。
\section{强化学习训练}
我们分别训练了骑士，皇后，\textbf{超级皇后}三种规则下的基于经典卷积模型并使用MCTS的强化学习AI。考虑到棋盘复杂度与设备算力限制，我们仅训练\textbf{超级皇后}的基于深度残差网络并使用MCTS的强化学习AI。
在训练过程中，我们设置超参数如下：
\paragraph{棋局对弈生成次数}
在每一轮新的迭代中，在自我对弈过程中使用前一轮训练出的网络指导蒙特卡洛树进行对弈生成更为稳健的对弈棋盘数据并对整个棋面局势数据集进行汰旧换新。我们将其设置为40，也就是说其将进行40次自我对弈，每次对弈开局为图\ref{fig:sstate}的任意一种开局；
\paragraph{MCTS模拟步数}
使用神经网络输出的概率分布与获胜预测值指导进行MCTS过程中的搜索模拟次数，即其完成选择-扩展-反向传播的次数。我们将其设置为50。
\paragraph{置信上界常数}
在选择动作（selection）中的常数$c$，用以平衡高胜率与低访问次数的关系，决定探索力度。我们将其设置为1。
\paragraph{对战次数}
更新后的神经网络与更新前的神经网络各自指导MCTS进行若干局对弈，若更新后的神经网络胜率超过$60\%$，则其将在下一轮训练中使用新的神经网络进行自我对弈并继续指导MCTS，继续增强走棋能力。否则继续使用更新前的神经网络。我们将其设置为40。
\paragraph{神经网络超参数}
优化器学习率（learning rate）设置为0.001，dropout率设置为0.3，epoch数设置为20，batch_size（批次容量）设置为128。

在数据生成时，为了使模型泛化能力更强，我们会对棋盘数据进行数据增强操作：对棋盘进行旋转与镜像，如图\ref{}。数据会因此被扩充8倍，大大增强数据集的鲁棒性。
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{datar.PNG}
    \caption[datar]{%
        数据增强%
      }
    \label{fig:datar}
\end{figure}
\section{强度测试}

\subsection{随机玩家}

\subsection{贪婪算法玩家}

\subsection{Alpha-beta剪枝玩家}

\subsection{人机对战}
在人机对战环节

