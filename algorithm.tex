\chapter{方法设计}
\label{chap:algorithm}

\section{棋盘规则}
超级皇后的棋盘继承国际象棋的规格，并且可以根据喜好扩大盘面（8x8，12x12，16x16等等）。在我们设计的API中，游戏开始时棋盘的大小可以任意自定义。受限于设备，AlphaZero框架的AI玩家只训练了8x8棋盘（经典棋盘）。因此在本文中我们主要关心8x8棋盘的结果。
超级皇后对战的规则也可以在我们设置的api中指定，如引言中所述，超级皇后走法规则可选三个规则：

（1）超级状态（super），拥有皇后和骑士的权限，可按此两种棋子落子规则任意移动；

（2）皇后状态（queen），可横竖对角线移动，不可越过移动直线上的对手；

（3）骑士状态，可走“日”字，即先向左（或右）走1格，再向上（或下）走2格；或先向左（或右）走2格，再向上（或下）走1格。

棋盘每个位置都具有四个状态值候选：$\{white:1, black:-1,empty:0,dead:99\}$，抛开初始位置（左上角与右下角），剩余位置的状态转换如下所示：
\begin{equation}
    \begin{aligned}
    empty &\stackrel{\mathrm{move}}{\longrightarrow} black \stackrel{\mathrm{leave}}{\longrightarrow} dead \\
    empty &\stackrel{\mathrm{move}}{\longrightarrow} white \stackrel{\mathrm{leave}}{\longrightarrow} dead 
    \end{aligned}
\end{equation}

依据信息暴露程度，超级皇后对战属于完美信息游戏（Perfect-Information Game）\cite{binmore2007game}。对于完美信息游戏，可以用状态空间复杂度（State-Space Complexity）和博弈树复杂度（Game-Tree Complexity）对其难度进行衡量\cite{allis1994searching,VANDENHERIK2002277}。
状态空间复杂度是指从初始状态开始，可以实现的所有符合规则的状态的集合总数，在实际情况中一般使用该数量的上界表示，例如在围棋中允许出现全白或全黑的极端情况。
博弈树复杂度表示所有不同游戏路径数目，常用合理估计的下界表示：$valids^{turn}$。 $valids$表示每回合平均合法移动数目，$turn$表示平均游戏长度\cite{10.1007/BF00992697}。
类似于国际象棋与围棋，我们可以给出超级皇后对战的状态空间与博弈树复杂度（仅考虑8x8棋盘），如图\ref{fig:complexity}
\begin{figure}[htb]
    \centering
    \includegraphics[width=1\textwidth]{complexity.png}
    \caption[complexity]{%
        超级皇后对战复杂度\cite{enwiki:complexity}%
      }
    \label{fig:complexity}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{knight.PNG}
    \caption[rules-knight]{%
        超级皇后对战游戏规则——骑士状态%
      }
    \label{fig:knight}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{queen.PNG}
    \caption[rules-queen]{%
        超级皇后对战游戏规则——皇后状态%
      }
    \label{fig:queen}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{super.PNG}
    \caption[rules-super]{%
        超级皇后对战游戏规则——超级状态%
      }
    \label{fig:super}
\end{figure}
骑士状态下，不论是Alpha-beta剪枝还是蒙特卡洛树搜索进行模拟速度都相对较快，因此在自我对弈阶段的落子速度也很快，但由于博弈树复杂度较低，启发式搜索算法的效果可能会好于基于蒙特卡洛树搜索的AlphaZero框架，因为其可以在较短时间内较小深度下搜索大部分个博弈树，其局部最优解更靠近全局最优解。
而超级状态下每回合平均合法移动数目相对较多，搜索代价较大，在同等限制下更加适合深度强化学习AlphaZero训练出的模型。

\section{基准玩家设计}
我们总共实现了4个AI玩家：随机玩家（Random Player），贪婪玩家(GreedyPlayer),Alpha-beta剪枝玩家, AlphaZero型玩家。前三个玩家将作为基准玩家（baseline player）。
\paragraph{随机玩家}
随机玩家，顾名思义，即为其每步动作都基于当前可行走法进行随机选择。在每一回合开始，随机玩家先输出自己的可选动作列表，并在此列表中随机选择一个作为自己在这一回合的动作。如图\ref{fig:super}，白色圆点代表白色方可行动作，随机玩家将从这些白色圆点里选择。
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{rp.PNG}
    \caption[rp]{%
        随机玩家在白色圆点中任选%
      }
    \label{fig:super}
\end{figure}
\paragraph{贪婪玩家}
贪婪玩家采取的是贪心算法\cite{introAlgo}。这是寻找问题最优解的常见方法，在问题的每个步骤都采取贪心原则，即选取当前状态下最好的选择（局部最优解）。因此我们需要一个评价函数来衡量棋盘状态对于玩家的好坏，经过比较与实验，定义评价函数如下：
\begin{equation}
    Score(player) = Count(LegalMoves(player)) - Count(LegalMoves(opponent))
\end{equation}
在某一回合，当前选手有若干个合法行动可选，我们需要评估出其在遍历每一个行动后棋盘盘面的好坏。评估盘面好坏使用上述评估函数，玩家在一步之后的合法行动数量减去对手合法行动数量的值越大，说明该步是当前玩家的最优解，将遍历后得到的分数分数进行排序选择最大值对应的行动即可。
假设当前是黑色方回合，局势如图\ref{fig:gp1}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{gp1.PNG}
    \caption[gp1]{%
        黑色圆点代表当前黑色超级皇后合法行动%
      }
    \label{fig:gp1}
\end{figure}
其有五个合法落子位置可选，我们分别计算假设黑色方落到各个位置后的分数如图\ref{fig:gp2}：
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{gp2.PNG}
    \caption[gp2]{%
        分数计算与走法选择%
      }
    \label{fig:gp2}
\end{figure}
黑色圆点代表仅属于黑色超级皇后的合法行动，白色圆点代表仅属于白色超级皇后的合法行动，红色圆点代表双方共享的合法行动，根据分数计算结果，选择分数为3的走法。
\paragraph{Alpha-beta剪枝玩家}
在剪枝搜索过程中，我们仍使用在贪婪玩家部分中定义的评价函数更新叶节点的值。考虑到博弈树过深引起的搜索时间代价\cite{sstextbook}，Alpha-beta剪枝玩家的深度设置为4。初始$\alpha$值设置为-99，$\beta$值设置为99。假设当前是黑色方回合，局势仍然如图\ref{fig:gp1}。搜索与剪枝过程可表现为图\ref{fig:abpde}。
\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{abp_design.PNG}
    \caption[abpde]{%
        超级皇后对战搜索与剪枝%
      }
    \label{fig:abpde}
\end{figure}

\section{深度强化学习玩家设计}
超级皇后对战是一款全新游戏，它没有人类专家棋谱与对弈棋局数据库供我们使用监督学习训练。这也意味着我们需要从白板状态训练出一个足够胜过基准玩家的AI玩家，因此我们将参考AlphaGo Zero与AlphaZero的框架，使用基于单一神经网络并纳入蒙特卡洛树搜索的策略迭代（policy iteration）对超级皇后对战进行训练\cite{Silver1140,Silver2017,Silver2016}。

相比于基准玩家，强化学习玩家只采取棋盘上的棋子与空位死位作为输入特征，不需要人为设计分数函数与杀棋特征。考虑到棋盘特征是二维的数据，且在强化学习过程中的自我对弈（self play）会生成大量数据，因此非常适合使用卷积神经网络对棋面局势进行评估以及对走棋动作进行选择\cite{Silver2016}。

\subsection{数据生成过程}
在自我对弈过程中，使用前一轮训练出的网络指导蒙特卡洛树进行对弈。对于每一局的每个回合t下的棋面局势$board_{t}$，蒙特卡洛树搜索模拟过程产生下一步走棋策略（概率分布）$\pi_{t}$并生成数据对$(board_{t},\pi_{t})$。
在每局结束时，对弈双方得到奖励为v（胜者为1，负者为-1），将此奖励值作为分类标签贴到在这一次对弈生成的数据对中产生$(board_{t},\pi_{t},v_{t})$，其中$v_{t}=\pm v$，正负符号当前棋局下棋方决定。
在训练时，$board_{t}$为输入网络的样本，棋局特征将从其二维结构中提取
\subsection{策略网络与值网络}
在原版的AlphaGo模型\cite{Silver2016}中，使用了两个网络，分别是策略网络（policy network）与值网络（value network），对应策略迭代算法中的策略函数与值函数。策略网络输出下一步动作的概率分布，即给出选择哪种动作的可能性更大；值网络评估当前棋面局势好坏，预测可能的获胜者。
但是AlphaGo的策略网络在一开始使用监督学习训练专家棋谱，再使用强化学习算法如策略梯度方法（policy gradient\cite{silver2014deterministic}），而其值网络从策略网络的自我对弈过程中学习并预测胜者。如前面所述，我们没有专家棋谱进行监督学习，因此参考AlphaGo Zero与AlphaZero使用单一网络输出概率分布与预测值是不错的选择。
我们采取以下两种不同结构的神经网络分别训练：
\paragraph{经典卷积神经网络}
其结构如图\ref{fig:cnn}所示：
（1）2D卷积层（适用图像的空间卷积），该层创建的卷积核对层输入进行卷积以生成输出张量，需要paddiong操作填充输入以使输出具有与原始输入相同的长度，其输出空间的维度为512；
（2）批量标准化层\cite{batchnorm}，在当前批次数据中标准化前面卷积层的激活项，特征轴设置为3；
（3）使用整流线性单位函数（Rectified Linear Unit, ReLU）\cite{xu2015empirical}激活一次，减少计算结果；
（4）以上三步再重复3次，在后两次中不需要进行padding操作；
（5）Flatten层将输入展平；
（6）全连接层，其输出空间维度为1024，在进行标准化后使用ReLU激活并使用Dropout\cite{srivastava2014dropout}防止过拟合；
（7）全连接层，其输出空间维度为512，在进行标准化后使用ReLU激活并使用Dropout防止过拟合；
（8）使用Softmax 激活函数的全连接层输出维度为动作空间（action size）的概率值；
（9）使用双曲正切激活函数（tanh）的全连接层输出维度为1的奖励值（预测胜者）；


\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{classic.png}
    \caption[cnn]{%
    经典卷积神经网络%
      }
    \label{fig:cnn}
\end{figure}



\subsection{策略迭代应用}
